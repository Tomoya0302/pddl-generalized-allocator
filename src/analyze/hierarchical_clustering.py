"""
éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import json

from .feature_extractor import extract_features_from_directory
from ..config.clustering_analysis_schema import ClusteringAnalysisConfig


class HierarchicalClusteringAnalyzer:
    """éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: ClusteringAnalysisConfig):
        self.config = config
        self.features = None
        self.feature_names = None
        self.solution_names = None
        self.normalized_features = None
        self.scaler = None
        self.cluster_labels = None
        self.linkage_matrix = None
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self.output_dir = Path(config.output.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def load_and_extract_features(self):
        """ç‰¹å¾´é‡ã®èª­ã¿è¾¼ã¿ã¨æŠ½å‡º"""
        print("ğŸ” Extracting features from solutions...")
        
        role_config_path = self.config.feature_extraction.role_config_file
        
        self.features, self.feature_names, self.solution_names = extract_features_from_directory(
            self.config.results_directory,
            self.config,
            role_config_path
        )
        
        if len(self.features) == 0:
            raise ValueError("No features extracted. Check the results directory and file pattern.")
        
        print(f"âœ… Extracted {len(self.feature_names)} features from {len(self.solution_names)} solutions")
        print(f"ğŸ“Š Features: {', '.join(self.feature_names[:5])}{'...' if len(self.feature_names) > 5 else ''}")
        
        # ç‰¹å¾´é‡ã‚’CSVã§ä¿å­˜
        if self.config.output.save_features_csv:
            features_df = pd.DataFrame(
                self.features,
                index=self.solution_names,
                columns=self.feature_names
            )
            features_csv_path = self.output_dir / f"{self.config.output.prefix}_features.csv"
            features_df.to_csv(features_csv_path)
            print(f"ğŸ’¾ Features saved to {features_csv_path}")
    
    def normalize_features(self):
        """ç‰¹å¾´é‡ã®æ­£è¦åŒ–"""
        if not self.config.feature_extraction.normalize_features:
            self.normalized_features = self.features
            return
        
        print("ğŸ“ Normalizing features...")
        
        method = self.config.feature_extraction.normalization_method
        if method == "standard":
            self.scaler = StandardScaler()
        elif method == "minmax":
            self.scaler = MinMaxScaler()
        elif method == "robust":
            self.scaler = RobustScaler()
        else:
            raise ValueError(f"Unknown normalization method: {method}")
        
        self.normalized_features = self.scaler.fit_transform(self.features)
        print(f"âœ… Features normalized using {method} scaling")
    
    def perform_clustering(self):
        """éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®å®Ÿè¡Œ"""
        print("ğŸ”— Performing hierarchical clustering...")
        
        # Linkage matrixè¨ˆç®—
        method = self.config.clustering.method
        metric = self.config.clustering.metric
        
        self.linkage_matrix = linkage(
            self.normalized_features,
            method=method,
            metric=metric
        )
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®æ±ºå®š
        if self.config.clustering.num_clusters:
            n_clusters = self.config.clustering.num_clusters
        else:
            n_clusters = self._determine_optimal_clusters()
        
        print(f"ğŸ“ˆ Using {n_clusters} clusters")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å‰²ã‚Šå½“ã¦
        self.cluster_labels = fcluster(
            self.linkage_matrix,
            n_clusters,
            criterion='maxclust'
        )
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å“è³ªè©•ä¾¡
        self._evaluate_clustering()
        
        print("âœ… Hierarchical clustering completed")
    
    def _determine_optimal_clusters(self) -> int:
        """æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’è‡ªå‹•æ±ºå®š"""
        method = self.config.clustering.auto_cluster_method
        max_k = min(self.config.clustering.max_clusters, len(self.solution_names) - 1)
        min_k = self.config.clustering.min_clusters
        
        print(f"ğŸ¯ Determining optimal number of clusters using {method}...")
        
        if method == "silhouette":
            best_k = self._find_best_k_silhouette(min_k, max_k)
        elif method == "elbow":
            best_k = self._find_best_k_elbow(min_k, max_k)
        elif method == "dendrogram_gap":
            best_k = self._find_best_k_dendrogram_gap(min_k, max_k)
        else:
            print(f"âš ï¸  Unknown method {method}, using silhouette")
            best_k = self._find_best_k_silhouette(min_k, max_k)
        
        return best_k
    
    def _find_best_k_silhouette(self, min_k: int, max_k: int) -> int:
        """ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°ã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°æ±ºå®š"""
        best_score = -1
        best_k = min_k
        
        for k in range(min_k, max_k + 1):
            labels = fcluster(self.linkage_matrix, k, criterion='maxclust')
            if len(set(labels)) > 1:  # æœ€ä½2ã¤ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒå¿…è¦
                score = silhouette_score(self.normalized_features, labels)
                if score > best_score:
                    best_score = score
                    best_k = k
        
        print(f"ğŸ¯ Best silhouette score: {best_score:.3f} with {best_k} clusters")
        return best_k
    
    def _find_best_k_elbow(self, min_k: int, max_k: int) -> int:
        """ã‚¨ãƒ«ãƒœãƒ¼æ³•ã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°æ±ºå®š"""
        inertias = []
        
        for k in range(min_k, max_k + 1):
            clustering = AgglomerativeClustering(n_clusters=k)
            labels = clustering.fit_predict(self.normalized_features)
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…å¹³å‡è·é›¢ã‚’è¨ˆç®—
            inertia = 0
            for cluster_id in set(labels):
                cluster_points = self.normalized_features[labels == cluster_id]
                if len(cluster_points) > 1:
                    cluster_center = cluster_points.mean(axis=0)
                    distances = np.sum((cluster_points - cluster_center) ** 2, axis=1)
                    inertia += distances.sum()
            
            inertias.append(inertia)
        
        # ã‚¨ãƒ«ãƒœãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        diffs = np.diff(inertias)
        elbow_k = min_k + np.argmax(diffs[:-1] - diffs[1:]) + 1
        
        print(f"ğŸ¯ Elbow method suggests {elbow_k} clusters")
        return elbow_k
    
    def _find_best_k_dendrogram_gap(self, min_k: int, max_k: int) -> int:
        """ãƒ‡ãƒ³ãƒ‰ãƒ­ã‚°ãƒ©ãƒ ã®ã‚®ãƒ£ãƒƒãƒ—ã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°æ±ºå®š"""
        # ãƒ‡ãƒ³ãƒ‰ãƒ­ã‚°ãƒ©ãƒ ã®é«˜ã•ã®å·®ã‚’è¨ˆç®—
        heights = self.linkage_matrix[:, 2]
        height_diffs = np.diff(heights[::-1])  # é€†é †ï¼ˆä¸Šã‹ã‚‰ä¸‹ã¸ï¼‰
        
        # æœ€å¤§ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’è¦‹ã¤ã‘ã‚‹
        max_gap_idx = np.argmax(height_diffs[:max_k-min_k])
        optimal_k = max_k - max_gap_idx
        
        # ç¯„å›²å†…ã«åã‚ã‚‹
        optimal_k = max(min_k, min(optimal_k, max_k))
        
        print(f"ğŸ¯ Dendrogram gap suggests {optimal_k} clusters")
        return optimal_k
    
    def _evaluate_clustering(self):
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®è©•ä¾¡"""
        if len(set(self.cluster_labels)) > 1:
            silhouette = silhouette_score(self.normalized_features, self.cluster_labels)
            calinski_harabasz = calinski_harabasz_score(self.normalized_features, self.cluster_labels)
            
            print(f"ğŸ“Š Clustering Quality:")
            print(f"   Silhouette Score: {silhouette:.3f}")
            print(f"   Calinski-Harabasz Score: {calinski_harabasz:.3f}")
    
    def create_visualizations(self):
        """å¯è¦–åŒ–ã®ä½œæˆ"""
        if not self.config.visualization.create_dendrogram and \
           not self.config.visualization.create_scatter_plots and \
           not self.config.visualization.create_feature_importance:
            return
        
        print("ğŸ“Š Creating visualizations...")
        
        # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        plt.style.use(self.config.visualization.style)
        colors = sns.color_palette(self.config.visualization.color_palette, 
                                 len(set(self.cluster_labels)))
        
        # ãƒ‡ãƒ³ãƒ‰ãƒ­ã‚°ãƒ©ãƒ 
        if self.config.visualization.create_dendrogram:
            self._create_dendrogram()
        
        # æ•£å¸ƒå›³
        if self.config.visualization.create_scatter_plots:
            self._create_scatter_plots(colors)
        
        # ç‰¹å¾´é‡é‡è¦åº¦
        if self.config.visualization.create_feature_importance:
            self._create_feature_importance_plot()
        
        print("âœ… Visualizations created")
    
    def _create_dendrogram(self):
        """ãƒ‡ãƒ³ãƒ‰ãƒ­ã‚°ãƒ©ãƒ ã®ä½œæˆ"""
        plt.figure(figsize=self.config.visualization.figure_size)
        
        dendrogram(
            self.linkage_matrix,
            labels=self.solution_names,
            orientation='top',
            distance_sort='descending',
            show_leaf_counts=True
        )
        
        plt.title('Hierarchical Clustering Dendrogram')
        plt.xlabel('Solution')
        plt.ylabel('Distance')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        
        if self.config.output.save_plots:
            dendrogram_path = self.output_dir / f"{self.config.output.prefix}_dendrogram.png"
            plt.savefig(dendrogram_path, dpi=self.config.visualization.dpi, bbox_inches='tight')
        
        # plt.show()
    
    def save_results(self):
        """çµæœã®ä¿å­˜"""
        print("ğŸ’¾ Saving clustering results...")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å‰²ã‚Šå½“ã¦ã®ä¿å­˜
        if self.config.output.save_cluster_assignments:
            cluster_assignments = pd.DataFrame({
                'solution': self.solution_names,
                'cluster': self.cluster_labels
            })
            assignments_path = self.output_dir / f"{self.config.output.prefix}_cluster_assignments.csv"
            cluster_assignments.to_csv(assignments_path, index=False)
        
        # è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
        if self.config.output.save_summary_report:
            self._create_summary_report()
        
        print(f"âœ… Results saved to {self.output_dir}")
    
    def _create_summary_report(self):
        """è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
        report_path = self.output_dir / f"{self.config.output.prefix}_summary.txt"
        
        with open(report_path, 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("HIERARCHICAL CLUSTERING ANALYSIS REPORT\n")
            f.write("=" * 80 + "\n\n")
            
            # åŸºæœ¬æƒ…å ±
            f.write(f"Total Solutions Analyzed: {len(self.solution_names)}\n")
            f.write(f"Number of Features: {len(self.feature_names)}\n")
            f.write(f"Number of Clusters: {len(set(self.cluster_labels))}\n")
            f.write(f"Clustering Method: {self.config.clustering.method}\n")
            f.write(f"Distance Metric: {self.config.clustering.metric}\n\n")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æƒ…å ±
            f.write("CLUSTER SUMMARY:\n")
            f.write("-" * 50 + "\n")
            
            from collections import Counter
            cluster_counts = Counter(self.cluster_labels)
            
            for cluster_id, count in sorted(cluster_counts.items()):
                f.write(f"Cluster {cluster_id}: {count} solutions\n")
                
                # ã“ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å±ã™ã‚‹è§£ã‚’ãƒªã‚¹ãƒˆ
                cluster_solutions = [name for name, label in zip(self.solution_names, self.cluster_labels)
                                   if label == cluster_id]
                f.write(f"  Solutions: {', '.join(cluster_solutions)}\n")
                
                # ã“ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ç‰¹å¾´é‡å¹³å‡
                cluster_mask = self.cluster_labels == cluster_id
                cluster_features = self.normalized_features[cluster_mask].mean(axis=0)
                
                # ä¸Šä½3ã¤ã®ç‰¹å¾´é‡
                top_feature_indices = np.argsort(np.abs(cluster_features))[-3:]
                f.write(f"  Top features: ")
                for idx in reversed(top_feature_indices):
                    feature_name = self.feature_names[idx]
                    feature_value = cluster_features[idx]
                    f.write(f"{feature_name}({feature_value:.2f}) ")
                f.write("\n\n")
            
            # ç‰¹å¾´é‡æƒ…å ±
            f.write("FEATURE SUMMARY:\n")
            f.write("-" * 50 + "\n")
            f.write(f"Features used: {', '.join(self.feature_names)}\n\n")
            
            # è¨­å®šæƒ…å ±
            f.write("CONFIGURATION:\n")
            f.write("-" * 50 + "\n")
            f.write(f"Results Directory: {self.config.results_directory}\n")
            f.write(f"Normalization: {self.config.feature_extraction.normalization_method}\n")
            f.write(f"Structural Features: {self.config.feature_extraction.use_structural_features}\n")
    
    def analyze(self):
        """å®Œå…¨ãªåˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: ç‰¹å¾´é‡æŠ½å‡º
            self.load_and_extract_features()
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: æ­£è¦åŒ–
            self.normalize_features()
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            self.perform_clustering()
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: å¯è¦–åŒ–
            self.create_visualizations()
            
            # ã‚¹ãƒ†ãƒƒãƒ—5: çµæœä¿å­˜
            self.save_results()
            
            print("ğŸ‰ Hierarchical clustering analysis completed successfully!")
            
            return {
                'cluster_labels': self.cluster_labels,
                'solution_names': self.solution_names,
                'feature_names': self.feature_names,
                'n_clusters': len(set(self.cluster_labels))
            }
            
        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            raise


    def _create_scatter_plots(self, colors):
        """æ•£å¸ƒå›³ã®ä½œæˆï¼ˆPCAä½¿ç”¨ï¼‰"""
        if self.normalized_features.shape[1] < 2:
            print("âš ï¸  Not enough features for scatter plot")
            return
        
        from sklearn.decomposition import PCA
        
        # PCAã§2æ¬¡å…ƒã«å‰Šæ¸›
        pca = PCA(n_components=2)
        features_2d = pca.fit_transform(self.normalized_features)
        
        plt.figure(figsize=self.config.visualization.figure_size)
        
        for i, cluster_id in enumerate(set(self.cluster_labels)):
            mask = self.cluster_labels == cluster_id
            plt.scatter(
                features_2d[mask, 0],
                features_2d[mask, 1],
                c=[colors[i]],
                label=f'Cluster {cluster_id}',
                s=100,
                alpha=0.7
            )
            
            # è§£ã®åå‰ã‚’ãƒ©ãƒ™ãƒ«ã¨ã—ã¦è¡¨ç¤º
            for j, (x, y) in enumerate(features_2d[mask]):
                solution_name = np.array(self.solution_names)[mask][j]
                plt.annotate(solution_name, (x, y), xytext=(5, 5),
                           textcoords='offset points', fontsize=8, alpha=0.8)
        
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
        plt.title('Solution Clusters (PCA projection)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        if self.config.output.save_plots:
            scatter_path = self.output_dir / f"{self.config.output.prefix}_scatter.png"
            plt.savefig(scatter_path, dpi=self.config.visualization.dpi, bbox_inches='tight')
        
        # plt.show()
    
    def _create_feature_importance_plot(self):
        """ç‰¹å¾´é‡é‡è¦åº¦ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ"""
        if len(set(self.cluster_labels)) <= 1:
            print("âš ï¸  Cannot compute feature importance with only one cluster")
            return
        
        # å„ç‰¹å¾´é‡ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é–“åˆ†æ•£ã‚’è¨ˆç®—
        feature_importance = []
        
        for i, feature_name in enumerate(self.feature_names):
            feature_values = self.normalized_features[:, i]
            cluster_means = []
            
            for cluster_id in set(self.cluster_labels):
                cluster_mask = self.cluster_labels == cluster_id
                cluster_mean = feature_values[cluster_mask].mean()
                cluster_means.append(cluster_mean)
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é–“ã®åˆ†æ•£ã‚’é‡è¦åº¦ã¨ã™ã‚‹
            importance = np.var(cluster_means)
            feature_importance.append((feature_name, importance))
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        feature_importance.sort(key=lambda x: x[1], reverse=True)
        
        # ä¸Šä½10å€‹ã‚’è¡¨ç¤º
        top_features = feature_importance[:10]
        
        plt.figure(figsize=(10, 6))
        feature_names_plot = [f[0] for f in top_features]
        importances = [f[1] for f in top_features]
        
        bars = plt.barh(range(len(top_features)), importances)
        plt.yticks(range(len(top_features)), feature_names_plot)
        plt.xlabel('Feature Importance (Inter-cluster variance)')
        plt.title('Top 10 Most Important Features for Clustering')
        plt.gca().invert_yaxis()
        
        # è‰²ä»˜ã‘
        colors_grad = plt.cm.viridis(np.linspace(0, 1, len(top_features)))
        for bar, color in zip(bars, colors_grad):
            bar.set_color(color)
        
        plt.tight_layout()
        
        if self.config.output.save_plots:
            importance_path = self.output_dir / f"{self.config.output.prefix}_feature_importance.png"
            plt.savefig(importance_path, dpi=self.config.visualization.dpi, bbox_inches='tight')
        
        # plt.show()


def run_clustering_analysis(config_path: str):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚’å®Ÿè¡Œ"""
    from ..config.clustering_analysis_schema import load_clustering_analysis_config
    
    config = load_clustering_analysis_config(config_path)
    analyzer = HierarchicalClusteringAnalyzer(config)
    
    return analyzer.analyze()
        